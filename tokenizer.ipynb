{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mGPT.archs.mgpt_vq import VQVae\n",
    "import time\n",
    "import moviepy.editor as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import os\n",
    "import time\n",
    "\n",
    "from textwrap import wrap\n",
    "from smplx import SMPL\n",
    "import imageio\n",
    "\n",
    "from mGPT.data.HumanML3D import HumanML3DDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SEED_VALUE': 1234, 'DEBUG': True, 'FULL_CONFIG': False, 'TRAIN': {'SPLIT': 'train', 'NUM_WORKERS': 16, 'BATCH_SIZE': 256, 'END_EPOCH': 300, 'RESUME': '', 'PRETRAINED_VAE': '', 'PRETRAINED': '', 'OPTIM': {'target': 'AdamW', 'params': {'lr': 0.0002, 'betas': [0.9, 0.99], 'weight_decay': 0.0}}, 'LR_SCHEDULER': {'target': 'CosineAnnealingLR', 'params': {'T_max': '${eval:${LOGGER.VAL_EVERY_STEPS} * 100}', 'eta_min': 1e-06}}, 'STAGE': 'vae'}, 'EVAL': {'SPLIT': 'test', 'BATCH_SIZE': 32, 'NUM_WORKERS': 8}, 'TEST': {'CHECKPOINTS': 'checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar', 'SPLIT': 'test', 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'SAVE_PREDICTIONS': False, 'COUNT_TIME': False, 'REPLICATION_TIMES': 20, 'REP_I': 0, 'FOLDER': 'results'}, 'model': {'target': 'mGPT.models.mgpt.MotionGPT', 'params': {'condition': 'text', 'task': 't2m', 'lm': '${lm.default}', 'motion_vae': '${vq.default}', 'stage': '${TRAIN.STAGE}', 'debug': '${DEBUG}', 'codebook_size': '${model.params.motion_vae.params.code_num}', 'metrics_dict': '${METRIC.TYPE}'}, 'whisper_path': 'deps/whisper-large-v2'}, 'LOSS': {'LAMBDA_REC': 1.0, 'LAMBDA_JOINT': 1.0, 'LAMBDA_LATENT': 1e-05, 'LAMBDA_KL': 1e-05, 'LAMBDA_GEN': 1.0, 'LAMBDA_CROSS': 1.0, 'LAMBDA_CYCLE': 1.0, 'LAMBDA_PRIOR': 0.0, 'LAMBDA_VELOCITY': 0.5, 'LAMBDA_COMMIT': 0.02, 'ABLATION': {'RECONS_LOSS': 'l1_smooth'}, 'LAMBDA_FEATURE': 1.0, 'LAMBDA_CLS': 1.0}, 'METRIC': {'TASK': 't2m', 'FORCE_IN_METER': True, 'DIST_SYNC_ON_STEP': True, 'MM_NUM_SAMPLES': 100, 'MM_NUM_REPEATS': 30, 'MM_NUM_TIMES': 10, 'DIVERSITY_TIMES': 300, 'TM2T': {'t2m_textencoder': {'target': 'mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo', 'params': {'word_size': 300, 'pos_size': 15, 'hidden_size': 512, 'output_size': 512}}, 't2m_moveencoder': {'target': 'mGPT.archs.tm2t_evaluator.MovementConvEncoder', 'params': {'input_size': '${eval:${DATASET.NFEATS} - 4}', 'hidden_size': 512, 'output_size': 512}}, 't2m_motionencoder': {'target': 'mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo', 'params': {'input_size': '${evaluator.tm2t.t2m_moveencoder.params.output_size}', 'hidden_size': 1024, 'output_size': 512}}, 't2m_path': 'deps/t2m/t2m/'}, 'TYPE': ['TM2TMetrics', 'MRMetrics']}, 'DATASET': {'target': 'mGPT.data.HumanML3D.HumanML3DDataModule', 'CODE_PATH': 'VQVAE', 'TASK_ROOT': 'deps/mGPT_instructions', 'TASK_PATH': '', 'NFEATS': 263, 'KIT': {'MAX_MOTION_LEN': 196, 'MIN_MOTION_LEN': 24, 'MAX_TEXT_LEN': 20, 'PICK_ONE_TEXT': True, 'FRAME_RATE': 12.5, 'UNIT_LEN': 4, 'ROOT': 'datasets/kit-ml', 'SPLIT_ROOT': 'datasets/kit-ml', 'MEAN_STD_PATH': 'deps/t2m/'}, 'HUMANML3D': {'MAX_MOTION_LEN': 196, 'MIN_MOTION_LEN': 40, 'MAX_TEXT_LEN': 20, 'PICK_ONE_TEXT': True, 'FRAME_RATE': 20.0, 'UNIT_LEN': 4, 'STD_TEXT': False, 'ROOT': './datasets/humanml3d', 'SPLIT_ROOT': './datasets/humanml3d', 'MEAN_STD_PATH': 'deps/t2m/'}, 'SMPL_PATH': 'deps/smpl', 'TRANSFORM_PATH': 'deps/transforms/', 'WORD_VERTILIZER_PATH': 'deps/t2m/glove/'}, 'ABLATION': {'use_length': False, 'predict_ratio': 0.2, 'inbetween_ratio': 0.25, 'image_size': 256, 'VAE_TYPE': 'actor', 'VAE_ARCH': 'encoder_decoder', 'PE_TYPE': 'actor', 'DIFF_PE_TYPE': 'actor', 'SKIP_CONNECT': False, 'MLP_DIST': False, 'IS_DIST': False, 'PREDICT_EPSILON': True}, 'DEMO': {'EXAMPLE': None, 'TASK': 't2m'}, 'LOGGER': {'VAL_EVERY_STEPS': 1, 'LOGGERS': ['tensorboard', 'wandb'], 'TENSORBOARD': {'target': 'pytorch_lightning.loggers.TensorBoardLogger', 'params': {'save_dir': '${FOLDER_EXP}', 'name': 'tensorboard', 'version': ''}}, 'WANDB': {'target': 'pytorch_lightning.loggers.WandbLogger', 'params': {'project': 'motiongpt', 'offline': True, 'id': None, 'version': '', 'name': '${NAME}', 'save_dir': '${FOLDER_EXP}', 'settings': {'init_timeout': 120}, 'log_model': True, 'mode': 'online', 'reinit': True}}, 'TYPE': ['wandb']}, 'NAME': 'debug--VQVAE_HumanML3D_full_training_baseline', 'ACCELERATOR': 'gpu', 'NUM_NODES': 1, 'DEVICE': [0, 1], 'codebook_experiments': {'config_h3d_stage1': {'NAME': 'Parallel_VQVAE_Feature_Extraction_RotationTrick_50_epochs', 'ACCELERATOR': 'gpu', 'NUM_NODES': 1, 'DEVICE': [0, 1, 2, 3], 'TRAIN': {'STAGE': 'vae', 'NUM_WORKERS': 16, 'BATCH_SIZE': 256, 'END_EPOCH': 50, 'RESUME': '', 'PRETRAINED': '', 'OPTIM': {'target': 'AdamW', 'params': {'lr': 0.0001, 'betas': [0.9, 0.99], 'weight_decay': 0.0}}}, 'EVAL': {'BATCH_SIZE': 32, 'SPLIT': 'test'}, 'TEST': {'CHECKPOINTS': 'checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar', 'SPLIT': 'test', 'BATCH_SIZE': 32}, 'DATASET': {'target': 'mGPT.data.HumanML3D.HumanML3DDataModule'}, 'METRIC': {'TYPE': ['TM2TMetrics', 'MRMetrics']}, 'LOSS': {'LAMBDA_FEATURE': 1.0, 'LAMBDA_VELOCITY': 0.5, 'LAMBDA_COMMIT': 0.02, 'LAMBDA_CLS': 1.0, 'ABLATION': {'RECONS_LOSS': 'l1_smooth'}}, 'model': {'target': 'mGPT.models.mgpt.MotionGPT', 'params': {'condition': 'text', 'task': 't2m', 'lm': '${lm.default}', 'motion_vae': '${vq.default}'}}, 'LOGGER': {'TYPE': ['wandb'], 'VAL_EVERY_STEPS': 10, 'WANDB': {'params': {'project': 'motiongpt', 'settings': {'init_timeout': 120}, 'log_model': True, 'mode': 'online', 'reinit': True}}}}}, 'evaluator': {'tm2t': {'t2m_textencoder': {'target': 'mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo', 'params': {'word_size': 300, 'pos_size': 15, 'hidden_size': 512, 'output_size': 512}}, 't2m_moveencoder': {'target': 'mGPT.archs.tm2t_evaluator.MovementConvEncoder', 'params': {'input_size': '${eval:${DATASET.NFEATS} - 4}', 'hidden_size': 512, 'output_size': 512}}, 't2m_motionencoder': {'target': 'mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo', 'params': {'input_size': '${evaluator.tm2t.t2m_moveencoder.params.output_size}', 'hidden_size': 1024, 'output_size': 512}}}}, 'lm': {'default': {'target': 'mGPT.archs.mgpt_lm.MLM', 'params': {'model_type': 't5', 'model_path': './deps/flan-t5-base', 'stage': '${TRAIN.STAGE}', 'motion_codebook_size': '${model.params.codebook_size}', 'ablation': '${ABLATION}'}}, 'gpt2_medium': {'target': 'mGPT.archs.mgpt_lm.MLM', 'params': {'model_type': 'gpt2', 'model_path': 'openai/gpt2-medium', 'stage': '${TRAIN.STAGE}', 'motion_codebook_size': '${model.params.codebook_size}', 'ablation': '${ABLATION}'}}, 't5_large': {'target': 'mGPT.archs.mgpt_lm.MLM', 'params': {'model_type': 't5', 'model_path': 'google/flan-t5-large', 'stage': '${TRAIN.STAGE}', 'motion_codebook_size': '${model.params.codebook_size}', 'ablation': '${ABLATION}'}}, 't5_small': {'target': 'mGPT.archs.mgpt_lm.MLM', 'params': {'model_type': 't5', 'model_path': 'google/flan-t5-small', 'stage': '${TRAIN.STAGE}', 'motion_codebook_size': '${model.params.codebook_size}', 'ablation': '${ABLATION}'}}}, 'vq': {'default': {'target': 'mGPT.archs.mgpt_vq.VQVae', 'params': {'quantizer': 'ema_reset', 'code_num': 512, 'code_dim': 512, 'output_emb_width': 512, 'down_t': 2, 'stride_t': 2, 'width': 512, 'depth': 3, 'dilation_growth_rate': 3, 'apply_rotation_trick': True, 'num_branches': 2, 'norm': 'None', 'activation': 'relu', 'nfeats': '${DATASET.NFEATS}', 'ablation': '${ABLATION}'}}}, 'CONFIG_FOLDER': 'configs', 'FOLDER': 'experiments', 'RENDER': {'BLENDER_PATH': 'libs/blender-2.93.2-linux-x64/blender', 'SMPL_MODEL_PATH': 'deps/smpl/smpl_models/smpl', 'MODEL_PATH': 'deps/smpl/smpl_models/', 'FACES_PATH': 'deps/smplh/smplh.faces'}}\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import glob\n",
    "\n",
    "# Create parser and add arguments manually\n",
    "parser = ArgumentParser()\n",
    "group = parser.add_argument_group(\"Training options\")\n",
    "\n",
    "# Add the arguments\n",
    "group.add_argument(\"--cfg_assets\", type=str, default=\"./configs/assets.yaml\")\n",
    "group.add_argument(\"--cfg\", type=str, default=\"./configs/config_h3d_stage1.yaml\")\n",
    "group.add_argument(\"--batch_size\", type=int, required=False)\n",
    "group.add_argument(\"--num_nodes\", type=int, required=False)\n",
    "group.add_argument(\"--device\", type=int, nargs=\"+\", required=False)\n",
    "group.add_argument(\"--task\", type=str, required=False)\n",
    "group.add_argument(\"--nodebug\", action=\"store_true\", required=False)\n",
    "\n",
    "# Parse args with default values\n",
    "params = parser.parse_args([])  # Empty list to avoid reading sys.argv\n",
    "\n",
    "# Register OmegaConf resolver if needed\n",
    "if not OmegaConf.has_resolver('eval'):\n",
    "    OmegaConf.register_new_resolver(\"eval\", eval)\n",
    "\n",
    "# Load config files\n",
    "cfg_assets = OmegaConf.load(params.cfg_assets)\n",
    "cfg_base = OmegaConf.load(pjoin(cfg_assets.CONFIG_FOLDER, 'default.yaml'))\n",
    "cfg_exp = OmegaConf.merge(cfg_base, OmegaConf.load(params.cfg))\n",
    "\n",
    "# Get module config if needed\n",
    "def get_module_config(cfg, filepath=\"./configs\"):\n",
    "    yamls = glob.glob(pjoin(filepath, '*', '*.yaml'))\n",
    "    yamls = [y.replace(filepath, '') for y in yamls]\n",
    "    for yaml in yamls:\n",
    "        nodes = yaml.replace('.yaml', '').replace(os.sep, '.')\n",
    "        nodes = nodes[1:] if nodes[0] == '.' else nodes\n",
    "        OmegaConf.update(cfg, nodes, OmegaConf.load('./configs' + yaml))\n",
    "    return cfg\n",
    "\n",
    "if not cfg_exp.FULL_CONFIG:\n",
    "    cfg_exp = get_module_config(cfg_exp, cfg_assets.CONFIG_FOLDER)\n",
    "\n",
    "# Merge configs\n",
    "cfg = OmegaConf.merge(cfg_exp, cfg_assets)\n",
    "\n",
    "# Update config with arguments\n",
    "cfg.TRAIN.BATCH_SIZE = params.batch_size if params.batch_size else cfg.TRAIN.BATCH_SIZE\n",
    "cfg.DEVICE = params.device if params.device else cfg.DEVICE\n",
    "cfg.NUM_NODES = params.num_nodes if params.num_nodes else cfg.NUM_NODES\n",
    "cfg.model.params.task = params.task if params.task else cfg.model.params.task\n",
    "cfg.DEBUG = not params.nodebug if params.nodebug is not None else cfg.DEBUG\n",
    "\n",
    "# Debug mode settings\n",
    "if cfg.DEBUG:\n",
    "    cfg.NAME = \"debug--\" + cfg.NAME\n",
    "    cfg.LOGGER.WANDB.params.offline = True\n",
    "    cfg.LOGGER.VAL_EVERY_STEPS = 1\n",
    "\n",
    "# Resume config if needed\n",
    "def resume_config(cfg):\n",
    "    if cfg.TRAIN.RESUME:\n",
    "        resume = cfg.TRAIN.RESUME\n",
    "        if os.path.exists(resume):\n",
    "            cfg.TRAIN.PRETRAINED = pjoin(resume, \"checkpoints\", \"last.ckpt\")\n",
    "            wandb_files = os.listdir(pjoin(resume, \"wandb\", \"latest-run\"))\n",
    "            wandb_run = [item for item in wandb_files if \"run-\" in item][0]\n",
    "            cfg.LOGGER.WANDB.params.id = wandb_run.replace(\"run-\",\"\").replace(\".wandb\", \"\")\n",
    "        else:\n",
    "            raise ValueError(\"Resume path is not right.\")\n",
    "    return cfg\n",
    "\n",
    "cfg = resume_config(cfg)\n",
    "\n",
    "# Now cfg contains your configuration\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    }
   ],
   "source": [
    "datamodule = HumanML3DDataModule(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    }
   ],
   "source": [
    "motion = datamodule.train_dataloader().dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/humanml3d/joints/000021.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m joints \u001b[38;5;241m=\u001b[39m datamodule\u001b[38;5;241m.\u001b[39mfeats2joints(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(motion)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m----> 3\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoints2feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m joints \u001b[38;5;241m=\u001b[39m datamodule\u001b[38;5;241m.\u001b[39mfeats2joints(feats\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "File \u001b[0;32m/workspace/canopy-labs/MotionGPT/mGPT/data/HumanML3D.py:89\u001b[0m, in \u001b[0;36mHumanML3DDataModule.joints2feats\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjoints2feats\u001b[39m(\u001b[38;5;28mself\u001b[39m, features):\n\u001b[0;32m---> 89\u001b[0m     example_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m000021.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     example_data \u001b[38;5;241m=\u001b[39m example_data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(example_data), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     91\u001b[0m     example_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(example_data)\n",
      "File \u001b[0;32m/workspace/canopy-labs/MotionGPT/.venv/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/humanml3d/joints/000021.npy'"
     ]
    }
   ],
   "source": [
    "joints = datamodule.feats2joints(torch.from_numpy(motion).to(torch.float32))\n",
    "\n",
    "feats = datamodule.joints2feats(joints)\n",
    "\n",
    "joints = datamodule.feats2joints(feats.to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.from_numpy(motion) - feats).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for output files\n",
    "import os\n",
    "os.makedirs('./outputs/original', exist_ok=True)\n",
    "os.makedirs('./outputs/joints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_motion(pos, output_dir='./outputs/original')\n",
    "render_motion(joints, output_dir='./outputs/joints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVae(\n",
    "        nfeats=263,\n",
    "        code_num=512,\n",
    "        code_dim=512,\n",
    "        output_emb_width=512,\n",
    "        apply_rotation_trick=False,\n",
    "        num_branches=1,\n",
    "        down_t=3,\n",
    "        datamodule=datamodule,\n",
    "        cfg=cfg,\n",
    "        debug=cfg.DEBUG if hasattr(cfg, 'DEBUG') else False,\n",
    "        metrics_dict=['MRMetrics', 'TM2TMetrics']\n",
    "    ).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'experiments/mgpt/VQVAE_HumanML3D_full_training_baseline/checkpoints/epoch=19.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(model_path, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 126 keys containing 'metric' or 't2m'\n"
     ]
    }
   ],
   "source": [
    "# Filter out keys that contain 'metric' or 't2m' in the name\n",
    "filtered_model_weights = {k: v for k, v in weights['state_dict'].items() \n",
    "                         if 'lm' not in k.lower() and 'metric' not in k.lower() and 't2m' not in k.lower()}\n",
    "\n",
    "# Keep the original state_dict structure\n",
    "state_dict = weights['state_dict'].copy()\n",
    "\n",
    "# Display the keys that are being filtered out\n",
    "filtered_out_keys = [k for k in weights['state_dict'].keys() \n",
    "                    if 'metric' in k.lower() or 't2m' in k.lower()]\n",
    "print(f\"Filtering out {len(filtered_out_keys)} keys containing 'metric' or 't2m'\")\n",
    "\n",
    "# Update the state_dict with only the filtered weights and remove 'vae.' prefix\n",
    "new_state_dict = {}\n",
    "for key in list(state_dict.keys()):\n",
    "    if key not in filtered_model_weights:\n",
    "        del state_dict[key]\n",
    "    else:\n",
    "        # Remove 'vae.' prefix from keys if present\n",
    "        new_key = key.replace('vae.', '')\n",
    "        new_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "# Replace the state_dict with the renamed keys\n",
    "state_dict = new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "motions = datamodule.train_dataloader().dataset[0][1]\n",
    "motions = torch.from_numpy(motions).to('cuda').to(torch.float32)\n",
    "motions = motions.view(1, -1, 263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstr, _, _ = model(motions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstr = datamodule.feats2joints(reconstr)\n",
    "motions = datamodule.feats2joints(motions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create base output directory if it doesn't exist\n",
    "base_output_dir = './outputs'\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "# Model path placeholder\n",
    "model_name = model_path.split('/')[2]\n",
    "\n",
    "# Process each pair of original and reconstructed motions\n",
    "for i in range(motions.shape[0]):\n",
    "    # Create directories named after each motion\n",
    "    motion_dir = os.path.join(base_output_dir, f\"motion_{i}\")\n",
    "    original_dir = os.path.join(motion_dir, \"original\")\n",
    "    recon_dir = os.path.join(motion_dir, \"reconstructed\")\n",
    "    os.makedirs(original_dir, exist_ok=True)\n",
    "    os.makedirs(recon_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract individual motions\n",
    "    original_motion = motions[i].unsqueeze(0).detach().cpu().numpy()\n",
    "    reconstructed_motion = reconstr[i].unsqueeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    # Create full output directory paths with proper path joining\n",
    "    original_output_dir = os.path.join(original_dir, f\"motion_{i}_{model_name}_original\")\n",
    "    recon_output_dir = os.path.join(recon_dir, f\"motion_{i}_{model_name}_reconstructed\")\n",
    "    \n",
    "    # Make sure these specific output directories exist\n",
    "    os.makedirs(original_output_dir, exist_ok=True)\n",
    "    os.makedirs(recon_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Render original motion with model info in filename\n",
    "    original_output = render_motion(\n",
    "        original_motion, \n",
    "        output_dir=original_output_dir,\n",
    "    )\n",
    "    \n",
    "    # Render reconstructed motion with model info in filename\n",
    "    recon_output = render_motion(\n",
    "        reconstructed_motion, \n",
    "        output_dir=recon_output_dir,\n",
    "    )\n",
    "    \n",
    "    # Print the output paths for reference\n",
    "    print(f\"Motion {i}:\")\n",
    "    print(f\"  Original: {original_output}\")\n",
    "    print(f\"  Reconstructed: {recon_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
