2025-02-21 16:42:19,588 SEED_VALUE: 1234
DEBUG: false
FULL_CONFIG: false
TRAIN:
  SPLIT: train
  NUM_WORKERS: 16
  BATCH_SIZE: 1024
  END_EPOCH: 50
  RESUME: ''
  PRETRAINED_VAE: ''
  PRETRAINED: ''
  OPTIM:
    target: AdamW
    params:
      lr: 0.0001
      betas:
      - 0.9
      - 0.99
      weight_decay: 0.0
  LR_SCHEDULER:
    target: CosineAnnealingLR
    params:
      T_max: ${eval:${LOGGER.VAL_EVERY_STEPS} * 100}
      eta_min: 1.0e-06
  STAGE: vae
EVAL:
  SPLIT: test
  BATCH_SIZE: 32
  NUM_WORKERS: 8
TEST:
  CHECKPOINTS: checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar
  SPLIT: test
  BATCH_SIZE: 32
  NUM_WORKERS: 8
  SAVE_PREDICTIONS: false
  COUNT_TIME: false
  REPLICATION_TIMES: 20
  REP_I: 0
  FOLDER: results
model:
  target: mGPT.models.mgpt.MotionGPT
  params:
    condition: text
    task: t2m
    lm: ${lm.default}
    motion_vae: ${vq.default}
    stage: ${TRAIN.STAGE}
    debug: ${DEBUG}
    codebook_size: ${model.params.motion_vae.params.code_num}
    metrics_dict: ${METRIC.TYPE}
  whisper_path: deps/whisper-large-v2
LOSS:
  LAMBDA_REC: 1.0
  LAMBDA_JOINT: 1.0
  LAMBDA_LATENT: 1.0e-05
  LAMBDA_KL: 1.0e-05
  LAMBDA_GEN: 1.0
  LAMBDA_CROSS: 1.0
  LAMBDA_CYCLE: 1.0
  LAMBDA_PRIOR: 0.0
  LAMBDA_VELOCITY: 0.5
  LAMBDA_COMMIT: 0.02
  ABLATION:
    RECONS_LOSS: l1_smooth
  LAMBDA_FEATURE: 1.0
  LAMBDA_CLS: 1.0
METRIC:
  TASK: t2m
  FORCE_IN_METER: true
  DIST_SYNC_ON_STEP: true
  MM_NUM_SAMPLES: 100
  MM_NUM_REPEATS: 30
  MM_NUM_TIMES: 10
  DIVERSITY_TIMES: 300
  TM2T:
    t2m_textencoder:
      target: mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo
      params:
        word_size: 300
        pos_size: 15
        hidden_size: 512
        output_size: 512
    t2m_moveencoder:
      target: mGPT.archs.tm2t_evaluator.MovementConvEncoder
      params:
        input_size: ${eval:${DATASET.NFEATS} - 4}
        hidden_size: 512
        output_size: 512
    t2m_motionencoder:
      target: mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo
      params:
        input_size: ${evaluator.tm2t.t2m_moveencoder.params.output_size}
        hidden_size: 1024
        output_size: 512
    t2m_path: deps/t2m/t2m/
  TYPE:
  - TM2TMetrics
  - MRMetrics
DATASET:
  target: mGPT.data.HumanML3D.HumanML3DDataModule
  CODE_PATH: VQVAE
  TASK_ROOT: deps/mGPT_instructions
  TASK_PATH: ''
  NFEATS: 263
  KIT:
    MAX_MOTION_LEN: 196
    MIN_MOTION_LEN: 24
    MAX_TEXT_LEN: 20
    PICK_ONE_TEXT: true
    FRAME_RATE: 12.5
    UNIT_LEN: 4
    ROOT: datasets/kit-ml
    SPLIT_ROOT: datasets/kit-ml
    MEAN_STD_PATH: deps/t2m/
  HUMANML3D:
    MAX_MOTION_LEN: 196
    MIN_MOTION_LEN: 40
    MAX_TEXT_LEN: 20
    PICK_ONE_TEXT: true
    FRAME_RATE: 20.0
    UNIT_LEN: 4
    STD_TEXT: false
    ROOT: /workspace/HumanML3D
    SPLIT_ROOT: /workspace/HumanML3D
    MEAN_STD_PATH: deps/t2m/
  SMPL_PATH: deps/smpl
  TRANSFORM_PATH: deps/transforms/
  WORD_VERTILIZER_PATH: deps/t2m/glove/
ABLATION:
  use_length: false
  predict_ratio: 0.2
  inbetween_ratio: 0.25
  image_size: 256
  VAE_TYPE: actor
  VAE_ARCH: encoder_decoder
  PE_TYPE: actor
  DIFF_PE_TYPE: actor
  SKIP_CONNECT: false
  MLP_DIST: false
  IS_DIST: false
  PREDICT_EPSILON: true
DEMO:
  EXAMPLE: null
  TASK: t2m
LOGGER:
  VAL_EVERY_STEPS: 10
  LOGGERS:
  - tensorboard
  - wandb
  TENSORBOARD:
    target: pytorch_lightning.loggers.TensorBoardLogger
    params:
      save_dir: ${FOLDER_EXP}
      name: tensorboard
      version: ''
  WANDB:
    target: pytorch_lightning.loggers.WandbLogger
    params:
      project: motiongpt
      offline: false
      id: null
      version: ''
      name: ${NAME}
      save_dir: ${FOLDER_EXP}
      settings:
        init_timeout: 120
      log_model: true
      mode: online
      reinit: true
  TYPE:
  - wandb
NAME: Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
ACCELERATOR: gpu
NUM_NODES: 1
DEVICE:
- 0
- 1
- 2
- 3
codebook_experiments:
  config_h3d_stage1:
    NAME: Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    - 1
    - 2
    - 3
    TRAIN:
      STAGE: vae
      NUM_WORKERS: 16
      BATCH_SIZE: 1024
      END_EPOCH: 50
      RESUME: ''
      PRETRAINED: ''
      OPTIM:
        target: AdamW
        params:
          lr: 0.0001
          betas:
          - 0.9
          - 0.99
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 32
      SPLIT: test
    TEST:
      CHECKPOINTS: checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar
      SPLIT: test
      BATCH_SIZE: 32
    DATASET:
      target: mGPT.data.HumanML3D.HumanML3DDataModule
    METRIC:
      TYPE:
      - TM2TMetrics
      - MRMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: mGPT.models.mgpt.MotionGPT
      params:
        condition: text
        task: t2m
        lm: ${lm.default}
        motion_vae: ${vq.default}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: motiongpt
          settings:
            init_timeout: 120
          log_model: true
          mode: online
          reinit: true
evaluator:
  tm2t:
    t2m_textencoder:
      target: mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo
      params:
        word_size: 300
        pos_size: 15
        hidden_size: 512
        output_size: 512
    t2m_moveencoder:
      target: mGPT.archs.tm2t_evaluator.MovementConvEncoder
      params:
        input_size: ${eval:${DATASET.NFEATS} - 4}
        hidden_size: 512
        output_size: 512
    t2m_motionencoder:
      target: mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo
      params:
        input_size: ${evaluator.tm2t.t2m_moveencoder.params.output_size}
        hidden_size: 1024
        output_size: 512
lm:
  default:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: t5
      model_path: ./deps/flan-t5-base
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
  gpt2_medium:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: gpt2
      model_path: openai/gpt2-medium
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
  t5_large:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: t5
      model_path: google/flan-t5-large
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
  t5_small:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: t5
      model_path: google/flan-t5-small
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
vq:
  default:
    target: mGPT.archs.mgpt_vq.VQVae
    params:
      quantizer: ema_reset
      code_num: 512
      code_dim: 512
      output_emb_width: 512
      down_t: 2
      stride_t: 2
      width: 512
      depth: 3
      dilation_growth_rate: 3
      apply_rotation_trick: true
      num_branches: 2
      norm: None
      activation: relu
      nfeats: ${DATASET.NFEATS}
      ablation: ${ABLATION}
CONFIG_FOLDER: configs
FOLDER: experiments
RENDER:
  BLENDER_PATH: libs/blender-2.93.2-linux-x64/blender
  SMPL_MODEL_PATH: deps/smpl/smpl_models/smpl
  MODEL_PATH: deps/smpl/smpl_models/
  FACES_PATH: deps/smplh/smplh.faces
FOLDER_EXP: experiments/mgpt/Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
TIME: 2025-02-21-16-42-19

Seed set to 1234
2025-02-21 16:42:19,622 Callbacks initialized
2025-02-21 16:42:20,332 datasets module HumanML3D initialized
2025-02-21 16:42:20,793 Created a temporary directory at /tmp/tmprq0vbkvz
2025-02-21 16:42:20,794 Writing /tmp/tmprq0vbkvz/_remote_module_non_scriptable.py
2025-02-21 16:42:32,035 model mGPT.models.mgpt.MotionGPT loaded
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
2025-02-21 16:42:32,050 Trainer initialized
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[rank: 0] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
[rank: 1] Seed set to 1234
[rank: 3] Seed set to 1234
[rank: 2] Seed set to 1234
[rank: 1] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[rank: 2] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
[rank: 3] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
2025-02-21 16:42:41,561 Added key: store_based_barrier_key:1 to store for rank: 0
2025-02-21 16:42:41,562 Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: irizar-xabier (irizar-xabier-finscout) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in experiments/mgpt/Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs/wandb/run-20250221_164243-xfstsz3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
wandb: â­ï¸ View project at https://wandb.ai/irizar-xabier-finscout/motiongpt
wandb: ğŸš€ View run at https://wandb.ai/irizar-xabier-finscout/motiongpt/runs/xfstsz3e
Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00
Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00


Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00



LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name    â”ƒ Type        â”ƒ Params â”ƒ Mode  â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ metrics â”‚ BaseMetrics â”‚ 65.1 M â”‚ train â”‚
â”‚ 1 â”‚ vae     â”‚ VQVae       â”‚ 42.2 M â”‚ train â”‚
â”‚ 2 â”‚ lm      â”‚ MLM         â”‚  248 M â”‚ train â”‚
â”‚ 3 â”‚ _losses â”‚ ModuleDict  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 290 M                                                         
Non-trainable params: 65.1 M                                                    
Total params: 355 M                                                             
Total estimated model params size (MB): 1.4 K                                   
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
2025-02-21 16:43:54,253 Sanity checking ok.
/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
2025-02-21 16:43:56,898 Training started
2025-02-21 16:44:02,669 Epoch 0: 
2025-02-21 16:44:12,144 Epoch 1: loss_total 1.004e+00
2025-02-21 16:44:21,612 Epoch 2: loss_total 9.863e-01
[rank: 0] Received SIGTERM: 15
pid 395625 killing 395849 with 15
pid 395625 killing 395850 with 15
pid 395625 killing 395851 with 15
Epoch 2/49 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0:00:05 â€¢ 0:00:00 0.95it/s 
Traceback (most recent call last):
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 210, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 324, in on_train_epoch_end
    self._save_last_checkpoint(trainer, monitor_candidates)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 694, in _save_last_checkpoint
    self._save_checkpoint(trainer, filepath)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 388, in _save_checkpoint
    trainer.save_checkpoint(filepath, self.save_weights_only)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1370, in save_checkpoint
    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 490, in save_checkpoint
    self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/lightning_fabric/plugins/io/torch_io.py", line 58, in save_checkpoint
    _atomic_save(checkpoint, path)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py", line 78, in _atomic_save
    torch.save(checkpoint, bytesbuffer)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/serialization.py", line 441, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/serialization.py", line 665, in _save
    storage = storage.cpu()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/storage.py", line 121, in cpu
    return torch.UntypedStorage(self.size()).copy_(self, False)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 397788) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/trial-week/MotionGPT/train.py", line 94, in <module>
    main()
  File "/root/trial-week/MotionGPT/train.py", line 85, in main
    trainer.fit(model, datamodule=datamodule)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1030, in _run_stage
    self.fit_loop.run()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 206, in run
    self.on_advance_end()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 378, in on_advance_end
    call._call_callback_hooks(trainer, "on_train_epoch_end", monitoring_callbacks=True)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 210, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 396915) is killed by signal: Terminated. 
Traceback (most recent call last):
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 210, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 324, in on_train_epoch_end
    self._save_last_checkpoint(trainer, monitor_candidates)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 694, in _save_last_checkpoint
    self._save_checkpoint(trainer, filepath)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 388, in _save_checkpoint
    trainer.save_checkpoint(filepath, self.save_weights_only)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1370, in save_checkpoint
    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 490, in save_checkpoint
    self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/lightning_fabric/plugins/io/torch_io.py", line 58, in save_checkpoint
    _atomic_save(checkpoint, path)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py", line 78, in _atomic_save
    torch.save(checkpoint, bytesbuffer)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/serialization.py", line 441, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/serialization.py", line 665, in _save
    storage = storage.cpu()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/storage.py", line 121, in cpu
    return torch.UntypedStorage(self.size()).copy_(self, False)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 397788) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/trial-week/MotionGPT/train.py", line 94, in <module>
    main()
  File "/root/trial-week/MotionGPT/train.py", line 85, in main
    trainer.fit(model, datamodule=datamodule)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1030, in _run_stage
    self.fit_loop.run()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 206, in run
    self.on_advance_end()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 378, in on_advance_end
    call._call_callback_hooks(trainer, "on_train_epoch_end", monitoring_callbacks=True)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 210, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 396915) is killed by signal: Terminated. 
[1;34mwandb[0m: 
[1;34mwandb[0m: ğŸš€ View run [33mParallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs[0m at: [34mhttps://wandb.ai/irizar-xabier-finscout/motiongpt/runs/xfstsz3e[0m
2025-02-21 16:45:57,557 SEED_VALUE: 1234
DEBUG: false
FULL_CONFIG: false
TRAIN:
  SPLIT: train
  NUM_WORKERS: 16
  BATCH_SIZE: 1024
  END_EPOCH: 500
  RESUME: ''
  PRETRAINED_VAE: ''
  PRETRAINED: ''
  OPTIM:
    target: AdamW
    params:
      lr: 0.0001
      betas:
      - 0.9
      - 0.99
      weight_decay: 0.0
  LR_SCHEDULER:
    target: CosineAnnealingLR
    params:
      T_max: ${eval:${LOGGER.VAL_EVERY_STEPS} * 100}
      eta_min: 1.0e-06
  STAGE: vae
EVAL:
  SPLIT: test
  BATCH_SIZE: 32
  NUM_WORKERS: 8
TEST:
  CHECKPOINTS: checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar
  SPLIT: test
  BATCH_SIZE: 32
  NUM_WORKERS: 8
  SAVE_PREDICTIONS: false
  COUNT_TIME: false
  REPLICATION_TIMES: 20
  REP_I: 0
  FOLDER: results
model:
  target: mGPT.models.mgpt.MotionGPT
  params:
    condition: text
    task: t2m
    lm: ${lm.default}
    motion_vae: ${vq.default}
    stage: ${TRAIN.STAGE}
    debug: ${DEBUG}
    codebook_size: ${model.params.motion_vae.params.code_num}
    metrics_dict: ${METRIC.TYPE}
  whisper_path: deps/whisper-large-v2
LOSS:
  LAMBDA_REC: 1.0
  LAMBDA_JOINT: 1.0
  LAMBDA_LATENT: 1.0e-05
  LAMBDA_KL: 1.0e-05
  LAMBDA_GEN: 1.0
  LAMBDA_CROSS: 1.0
  LAMBDA_CYCLE: 1.0
  LAMBDA_PRIOR: 0.0
  LAMBDA_VELOCITY: 0.5
  LAMBDA_COMMIT: 0.02
  ABLATION:
    RECONS_LOSS: l1_smooth
  LAMBDA_FEATURE: 1.0
  LAMBDA_CLS: 1.0
METRIC:
  TASK: t2m
  FORCE_IN_METER: true
  DIST_SYNC_ON_STEP: true
  MM_NUM_SAMPLES: 100
  MM_NUM_REPEATS: 30
  MM_NUM_TIMES: 10
  DIVERSITY_TIMES: 300
  TM2T:
    t2m_textencoder:
      target: mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo
      params:
        word_size: 300
        pos_size: 15
        hidden_size: 512
        output_size: 512
    t2m_moveencoder:
      target: mGPT.archs.tm2t_evaluator.MovementConvEncoder
      params:
        input_size: ${eval:${DATASET.NFEATS} - 4}
        hidden_size: 512
        output_size: 512
    t2m_motionencoder:
      target: mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo
      params:
        input_size: ${evaluator.tm2t.t2m_moveencoder.params.output_size}
        hidden_size: 1024
        output_size: 512
    t2m_path: deps/t2m/t2m/
  TYPE:
  - TM2TMetrics
  - MRMetrics
DATASET:
  target: mGPT.data.HumanML3D.HumanML3DDataModule
  CODE_PATH: VQVAE
  TASK_ROOT: deps/mGPT_instructions
  TASK_PATH: ''
  NFEATS: 263
  KIT:
    MAX_MOTION_LEN: 196
    MIN_MOTION_LEN: 24
    MAX_TEXT_LEN: 20
    PICK_ONE_TEXT: true
    FRAME_RATE: 12.5
    UNIT_LEN: 4
    ROOT: datasets/kit-ml
    SPLIT_ROOT: datasets/kit-ml
    MEAN_STD_PATH: deps/t2m/
  HUMANML3D:
    MAX_MOTION_LEN: 196
    MIN_MOTION_LEN: 40
    MAX_TEXT_LEN: 20
    PICK_ONE_TEXT: true
    FRAME_RATE: 20.0
    UNIT_LEN: 4
    STD_TEXT: false
    ROOT: /workspace/HumanML3D
    SPLIT_ROOT: /workspace/HumanML3D
    MEAN_STD_PATH: deps/t2m/
  SMPL_PATH: deps/smpl
  TRANSFORM_PATH: deps/transforms/
  WORD_VERTILIZER_PATH: deps/t2m/glove/
ABLATION:
  use_length: false
  predict_ratio: 0.2
  inbetween_ratio: 0.25
  image_size: 256
  VAE_TYPE: actor
  VAE_ARCH: encoder_decoder
  PE_TYPE: actor
  DIFF_PE_TYPE: actor
  SKIP_CONNECT: false
  MLP_DIST: false
  IS_DIST: false
  PREDICT_EPSILON: true
DEMO:
  EXAMPLE: null
  TASK: t2m
LOGGER:
  VAL_EVERY_STEPS: 10
  LOGGERS:
  - tensorboard
  - wandb
  TENSORBOARD:
    target: pytorch_lightning.loggers.TensorBoardLogger
    params:
      save_dir: ${FOLDER_EXP}
      name: tensorboard
      version: ''
  WANDB:
    target: pytorch_lightning.loggers.WandbLogger
    params:
      project: motiongpt
      offline: false
      id: null
      version: ''
      name: ${NAME}
      save_dir: ${FOLDER_EXP}
      settings:
        init_timeout: 120
      log_model: true
      mode: online
      reinit: true
  TYPE:
  - wandb
NAME: Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
ACCELERATOR: gpu
NUM_NODES: 1
DEVICE:
- 0
- 1
- 2
- 3
codebook_experiments:
  config_h3d_stage1:
    NAME: Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    - 1
    - 2
    - 3
    TRAIN:
      STAGE: vae
      NUM_WORKERS: 16
      BATCH_SIZE: 1024
      END_EPOCH: 500
      RESUME: ''
      PRETRAINED: ''
      OPTIM:
        target: AdamW
        params:
          lr: 0.0001
          betas:
          - 0.9
          - 0.99
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 32
      SPLIT: test
    TEST:
      CHECKPOINTS: checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar
      SPLIT: test
      BATCH_SIZE: 32
    DATASET:
      target: mGPT.data.HumanML3D.HumanML3DDataModule
    METRIC:
      TYPE:
      - TM2TMetrics
      - MRMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: mGPT.models.mgpt.MotionGPT
      params:
        condition: text
        task: t2m
        lm: ${lm.default}
        motion_vae: ${vq.default}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: motiongpt
          settings:
            init_timeout: 120
          log_model: true
          mode: online
          reinit: true
evaluator:
  tm2t:
    t2m_textencoder:
      target: mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo
      params:
        word_size: 300
        pos_size: 15
        hidden_size: 512
        output_size: 512
    t2m_moveencoder:
      target: mGPT.archs.tm2t_evaluator.MovementConvEncoder
      params:
        input_size: ${eval:${DATASET.NFEATS} - 4}
        hidden_size: 512
        output_size: 512
    t2m_motionencoder:
      target: mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo
      params:
        input_size: ${evaluator.tm2t.t2m_moveencoder.params.output_size}
        hidden_size: 1024
        output_size: 512
lm:
  default:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: t5
      model_path: ./deps/flan-t5-base
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
  gpt2_medium:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: gpt2
      model_path: openai/gpt2-medium
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
  t5_large:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: t5
      model_path: google/flan-t5-large
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
  t5_small:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: t5
      model_path: google/flan-t5-small
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
vq:
  default:
    target: mGPT.archs.mgpt_vq.VQVae
    params:
      quantizer: ema_reset
      code_num: 512
      code_dim: 512
      output_emb_width: 512
      down_t: 2
      stride_t: 2
      width: 512
      depth: 3
      dilation_growth_rate: 3
      apply_rotation_trick: true
      num_branches: 2
      norm: None
      activation: relu
      nfeats: ${DATASET.NFEATS}
      ablation: ${ABLATION}
CONFIG_FOLDER: configs
FOLDER: experiments
RENDER:
  BLENDER_PATH: libs/blender-2.93.2-linux-x64/blender
  SMPL_MODEL_PATH: deps/smpl/smpl_models/smpl
  MODEL_PATH: deps/smpl/smpl_models/
  FACES_PATH: deps/smplh/smplh.faces
FOLDER_EXP: experiments/mgpt/Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
TIME: 2025-02-21-16-45-57

Seed set to 1234
2025-02-21 16:45:57,594 Callbacks initialized
2025-02-21 16:45:58,295 datasets module HumanML3D initialized
2025-02-21 16:45:58,757 Created a temporary directory at /tmp/tmp_tohvxhg
2025-02-21 16:45:58,758 Writing /tmp/tmp_tohvxhg/_remote_module_non_scriptable.py
2025-02-21 16:46:10,079 model mGPT.models.mgpt.MotionGPT loaded
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
2025-02-21 16:46:10,094 Trainer initialized
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[rank: 0] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
[rank: 3] Seed set to 1234
[rank: 1] Seed set to 1234
[rank: 2] Seed set to 1234
[rank: 3] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[rank: 1] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[rank: 2] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
2025-02-21 16:46:19,998 Added key: store_based_barrier_key:1 to store for rank: 0
2025-02-21 16:46:20,000 Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: irizar-xabier (irizar-xabier-finscout) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in experiments/mgpt/Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs/wandb/run-20250221_164623-cehwcc2a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
wandb: â­ï¸ View project at https://wandb.ai/irizar-xabier-finscout/motiongpt
wandb: ğŸš€ View run at https://wandb.ai/irizar-xabier-finscout/motiongpt/runs/cehwcc2a
Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00

Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00
Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00
Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00
Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00

Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name    â”ƒ Type        â”ƒ Params â”ƒ Mode  â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ metrics â”‚ BaseMetrics â”‚ 65.1 M â”‚ train â”‚
â”‚ 1 â”‚ vae     â”‚ VQVae       â”‚ 42.2 M â”‚ train â”‚
â”‚ 2 â”‚ lm      â”‚ MLM         â”‚  248 M â”‚ train â”‚
â”‚ 3 â”‚ _losses â”‚ ModuleDict  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 290 M                                                         
Non-trainable params: 65.1 M                                                    
Total params: 355 M                                                             
Total estimated model params size (MB): 1.4 K                                   
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
2025-02-21 16:47:04,832 Sanity checking ok.
/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
2025-02-21 16:47:11,203 Training started
Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/trial-week/MotionGPT/train.py", line 94, in <module>
    main()
  File "/root/trial-week/MotionGPT/train.py", line 85, in main
    trainer.fit(model, datamodule=datamodule)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1030, in _run_stage
    self.fit_loop.run()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
    self._optimizer_step(batch_idx, closure)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
    call._call_lightning_module_hook(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 159, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1308, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 153, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 238, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 148, in step
    loss = closure()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 317, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 311, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 389, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 640, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 633, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/root/trial-week/MotionGPT/mGPT/models/base.py", line 25, in training_step
    return self.allsplit_step("train", batch, batch_idx)
  File "/root/trial-week/MotionGPT/mGPT/models/mgpt.py", line 377, in allsplit_step
    rs_set = self.train_vae_forward(batch)
  File "/root/trial-week/MotionGPT/mGPT/models/mgpt.py", line 307, in train_vae_forward
    feats_rst, loss_commit, perplexity = self.vae(feats_ref)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/mgpt_vq.py", line 89, in forward
    x_encoder = self.encoder(x_in)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/mgpt_vq.py", line 241, in forward
    out2 = self.branch2(x)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/tools/resnet.py", line 82, in forward
    return self.model(x)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/tools/resnet.py", line 68, in forward
    x = x + x_orig
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 3; 19.70 GiB total capacity; 3.74 GiB already allocated; 20.44 MiB free; 4.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/trial-week/MotionGPT/train.py", line 94, in <module>
    main()
  File "/root/trial-week/MotionGPT/train.py", line 85, in main
    trainer.fit(model, datamodule=datamodule)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1030, in _run_stage
    self.fit_loop.run()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
    self._optimizer_step(batch_idx, closure)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
    call._call_lightning_module_hook(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 159, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1308, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 153, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 238, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 148, in step
    loss = closure()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 317, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 311, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 389, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 640, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 633, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/root/trial-week/MotionGPT/mGPT/models/base.py", line 25, in training_step
    return self.allsplit_step("train", batch, batch_idx)
  File "/root/trial-week/MotionGPT/mGPT/models/mgpt.py", line 377, in allsplit_step
    rs_set = self.train_vae_forward(batch)
  File "/root/trial-week/MotionGPT/mGPT/models/mgpt.py", line 307, in train_vae_forward
    feats_rst, loss_commit, perplexity = self.vae(feats_ref)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/mgpt_vq.py", line 89, in forward
    x_encoder = self.encoder(x_in)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/mgpt_vq.py", line 241, in forward
    out2 = self.branch2(x)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/tools/resnet.py", line 82, in forward
    return self.model(x)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/tools/resnet.py", line 68, in forward
    x = x + x_orig
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 2; 19.70 GiB total capacity; 3.68 GiB already allocated; 6.44 MiB free; 4.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/trial-week/MotionGPT/train.py", line 94, in <module>
    main()
  File "/root/trial-week/MotionGPT/train.py", line 85, in main
    trainer.fit(model, datamodule=datamodule)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1030, in _run_stage
    self.fit_loop.run()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
    self._optimizer_step(batch_idx, closure)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
    call._call_lightning_module_hook(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 159, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1308, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 153, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 238, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 148, in step
    loss = closure()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 317, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 311, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 389, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 640, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 633, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/root/trial-week/MotionGPT/mGPT/models/base.py", line 25, in training_step
    return self.allsplit_step("train", batch, batch_idx)
  File "/root/trial-week/MotionGPT/mGPT/models/mgpt.py", line 377, in allsplit_step
    rs_set = self.train_vae_forward(batch)
  File "/root/trial-week/MotionGPT/mGPT/models/mgpt.py", line 307, in train_vae_forward
    feats_rst, loss_commit, perplexity = self.vae(feats_ref)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/mgpt_vq.py", line 89, in forward
    x_encoder = self.encoder(x_in)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/mgpt_vq.py", line 241, in forward
    out2 = self.branch2(x)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/tools/resnet.py", line 82, in forward
    return self.model(x)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/trial-week/MotionGPT/mGPT/archs/tools/resnet.py", line 68, in forward
    x = x + x_orig
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 1; 19.70 GiB total capacity; 3.68 GiB already allocated; 6.44 MiB free; 4.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[rank: 1] Child process with PID 401101 terminated with code 1. Forcefully terminating all other processes to avoid zombies ğŸ§Ÿ
2025-02-21 16:49:15,086 SEED_VALUE: 1234
DEBUG: false
FULL_CONFIG: false
TRAIN:
  SPLIT: train
  NUM_WORKERS: 16
  BATCH_SIZE: 1024
  END_EPOCH: 500
  RESUME: ''
  PRETRAINED_VAE: ''
  PRETRAINED: ''
  OPTIM:
    target: AdamW
    params:
      lr: 0.0001
      betas:
      - 0.9
      - 0.99
      weight_decay: 0.0
  LR_SCHEDULER:
    target: CosineAnnealingLR
    params:
      T_max: ${eval:${LOGGER.VAL_EVERY_STEPS} * 100}
      eta_min: 1.0e-06
  STAGE: vae
EVAL:
  SPLIT: test
  BATCH_SIZE: 32
  NUM_WORKERS: 8
TEST:
  CHECKPOINTS: checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar
  SPLIT: test
  BATCH_SIZE: 32
  NUM_WORKERS: 8
  SAVE_PREDICTIONS: false
  COUNT_TIME: false
  REPLICATION_TIMES: 20
  REP_I: 0
  FOLDER: results
model:
  target: mGPT.models.mgpt.MotionGPT
  params:
    condition: text
    task: t2m
    lm: ${lm.default}
    motion_vae: ${vq.default}
    stage: ${TRAIN.STAGE}
    debug: ${DEBUG}
    codebook_size: ${model.params.motion_vae.params.code_num}
    metrics_dict: ${METRIC.TYPE}
  whisper_path: deps/whisper-large-v2
LOSS:
  LAMBDA_REC: 1.0
  LAMBDA_JOINT: 1.0
  LAMBDA_LATENT: 1.0e-05
  LAMBDA_KL: 1.0e-05
  LAMBDA_GEN: 1.0
  LAMBDA_CROSS: 1.0
  LAMBDA_CYCLE: 1.0
  LAMBDA_PRIOR: 0.0
  LAMBDA_VELOCITY: 0.5
  LAMBDA_COMMIT: 0.02
  ABLATION:
    RECONS_LOSS: l1_smooth
  LAMBDA_FEATURE: 1.0
  LAMBDA_CLS: 1.0
METRIC:
  TASK: t2m
  FORCE_IN_METER: true
  DIST_SYNC_ON_STEP: true
  MM_NUM_SAMPLES: 100
  MM_NUM_REPEATS: 30
  MM_NUM_TIMES: 10
  DIVERSITY_TIMES: 300
  TM2T:
    t2m_textencoder:
      target: mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo
      params:
        word_size: 300
        pos_size: 15
        hidden_size: 512
        output_size: 512
    t2m_moveencoder:
      target: mGPT.archs.tm2t_evaluator.MovementConvEncoder
      params:
        input_size: ${eval:${DATASET.NFEATS} - 4}
        hidden_size: 512
        output_size: 512
    t2m_motionencoder:
      target: mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo
      params:
        input_size: ${evaluator.tm2t.t2m_moveencoder.params.output_size}
        hidden_size: 1024
        output_size: 512
    t2m_path: deps/t2m/t2m/
  TYPE:
  - TM2TMetrics
  - MRMetrics
DATASET:
  target: mGPT.data.HumanML3D.HumanML3DDataModule
  CODE_PATH: VQVAE
  TASK_ROOT: deps/mGPT_instructions
  TASK_PATH: ''
  NFEATS: 263
  KIT:
    MAX_MOTION_LEN: 196
    MIN_MOTION_LEN: 24
    MAX_TEXT_LEN: 20
    PICK_ONE_TEXT: true
    FRAME_RATE: 12.5
    UNIT_LEN: 4
    ROOT: datasets/kit-ml
    SPLIT_ROOT: datasets/kit-ml
    MEAN_STD_PATH: deps/t2m/
  HUMANML3D:
    MAX_MOTION_LEN: 196
    MIN_MOTION_LEN: 40
    MAX_TEXT_LEN: 20
    PICK_ONE_TEXT: true
    FRAME_RATE: 20.0
    UNIT_LEN: 4
    STD_TEXT: false
    ROOT: /workspace/HumanML3D
    SPLIT_ROOT: /workspace/HumanML3D
    MEAN_STD_PATH: deps/t2m/
  SMPL_PATH: deps/smpl
  TRANSFORM_PATH: deps/transforms/
  WORD_VERTILIZER_PATH: deps/t2m/glove/
ABLATION:
  use_length: false
  predict_ratio: 0.2
  inbetween_ratio: 0.25
  image_size: 256
  VAE_TYPE: actor
  VAE_ARCH: encoder_decoder
  PE_TYPE: actor
  DIFF_PE_TYPE: actor
  SKIP_CONNECT: false
  MLP_DIST: false
  IS_DIST: false
  PREDICT_EPSILON: true
DEMO:
  EXAMPLE: null
  TASK: t2m
LOGGER:
  VAL_EVERY_STEPS: 10
  LOGGERS:
  - tensorboard
  - wandb
  TENSORBOARD:
    target: pytorch_lightning.loggers.TensorBoardLogger
    params:
      save_dir: ${FOLDER_EXP}
      name: tensorboard
      version: ''
  WANDB:
    target: pytorch_lightning.loggers.WandbLogger
    params:
      project: motiongpt
      offline: false
      id: null
      version: ''
      name: ${NAME}
      save_dir: ${FOLDER_EXP}
      settings:
        init_timeout: 120
      log_model: true
      mode: online
      reinit: true
  TYPE:
  - wandb
NAME: Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
ACCELERATOR: gpu
NUM_NODES: 1
DEVICE:
- 0
- 1
- 2
- 3
codebook_experiments:
  config_h3d_stage1:
    NAME: Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
    ACCELERATOR: gpu
    NUM_NODES: 1
    DEVICE:
    - 0
    - 1
    - 2
    - 3
    TRAIN:
      STAGE: vae
      NUM_WORKERS: 16
      BATCH_SIZE: 1024
      END_EPOCH: 500
      RESUME: ''
      PRETRAINED: ''
      OPTIM:
        target: AdamW
        params:
          lr: 0.0001
          betas:
          - 0.9
          - 0.99
          weight_decay: 0.0
    EVAL:
      BATCH_SIZE: 32
      SPLIT: test
    TEST:
      CHECKPOINTS: checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar
      SPLIT: test
      BATCH_SIZE: 32
    DATASET:
      target: mGPT.data.HumanML3D.HumanML3DDataModule
    METRIC:
      TYPE:
      - TM2TMetrics
      - MRMetrics
    LOSS:
      LAMBDA_FEATURE: 1.0
      LAMBDA_VELOCITY: 0.5
      LAMBDA_COMMIT: 0.02
      LAMBDA_CLS: 1.0
      ABLATION:
        RECONS_LOSS: l1_smooth
    model:
      target: mGPT.models.mgpt.MotionGPT
      params:
        condition: text
        task: t2m
        lm: ${lm.default}
        motion_vae: ${vq.default}
    LOGGER:
      TYPE:
      - wandb
      VAL_EVERY_STEPS: 10
      WANDB:
        params:
          project: motiongpt
          settings:
            init_timeout: 120
          log_model: true
          mode: online
          reinit: true
evaluator:
  tm2t:
    t2m_textencoder:
      target: mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo
      params:
        word_size: 300
        pos_size: 15
        hidden_size: 512
        output_size: 512
    t2m_moveencoder:
      target: mGPT.archs.tm2t_evaluator.MovementConvEncoder
      params:
        input_size: ${eval:${DATASET.NFEATS} - 4}
        hidden_size: 512
        output_size: 512
    t2m_motionencoder:
      target: mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo
      params:
        input_size: ${evaluator.tm2t.t2m_moveencoder.params.output_size}
        hidden_size: 1024
        output_size: 512
lm:
  default:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: t5
      model_path: ./deps/flan-t5-base
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
  gpt2_medium:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: gpt2
      model_path: openai/gpt2-medium
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
  t5_large:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: t5
      model_path: google/flan-t5-large
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
  t5_small:
    target: mGPT.archs.mgpt_lm.MLM
    params:
      model_type: t5
      model_path: google/flan-t5-small
      stage: ${TRAIN.STAGE}
      motion_codebook_size: ${model.params.codebook_size}
      ablation: ${ABLATION}
vq:
  default:
    target: mGPT.archs.mgpt_vq.VQVae
    params:
      quantizer: ema_reset
      code_num: 512
      code_dim: 512
      output_emb_width: 512
      down_t: 2
      stride_t: 2
      width: 512
      depth: 3
      dilation_growth_rate: 3
      apply_rotation_trick: true
      num_branches: 2
      norm: None
      activation: relu
      nfeats: ${DATASET.NFEATS}
      ablation: ${ABLATION}
CONFIG_FOLDER: configs
FOLDER: experiments
RENDER:
  BLENDER_PATH: libs/blender-2.93.2-linux-x64/blender
  SMPL_MODEL_PATH: deps/smpl/smpl_models/smpl
  MODEL_PATH: deps/smpl/smpl_models/
  FACES_PATH: deps/smplh/smplh.faces
FOLDER_EXP: experiments/mgpt/Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
TIME: 2025-02-21-16-49-15

Seed set to 1234
2025-02-21 16:49:15,119 Callbacks initialized
2025-02-21 16:49:15,825 datasets module HumanML3D initialized
2025-02-21 16:49:16,311 Created a temporary directory at /tmp/tmpi4fij7yc
2025-02-21 16:49:16,312 Writing /tmp/tmpi4fij7yc/_remote_module_non_scriptable.py
2025-02-21 16:49:26,797 model mGPT.models.mgpt.MotionGPT loaded
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
2025-02-21 16:49:26,812 Trainer initialized
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[rank: 0] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
[rank: 1] Seed set to 1234
[rank: 3] Seed set to 1234
[rank: 2] Seed set to 1234
[rank: 1] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[rank: 3] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[rank: 2] Seed set to 1234
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
2025-02-21 16:49:36,541 Added key: store_based_barrier_key:1 to store for rank: 0
2025-02-21 16:49:36,542 Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: irizar-xabier (irizar-xabier-finscout) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in experiments/mgpt/Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs/wandb/run-20250221_164938-7yhuq4v6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs
wandb: â­ï¸ View project at https://wandb.ai/irizar-xabier-finscout/motiongpt
wandb: ğŸš€ View run at https://wandb.ai/irizar-xabier-finscout/motiongpt/runs/7yhuq4v6
Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00Loading full config...
pytorch_lightning.loggers WandbLogger
mGPT.data.HumanML3D HumanML3DDataModule
Pointer Pointing at 0
mGPT.models.mgpt MotionGPT
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.tm2t_evaluator TextEncoderBiGRUCo
mGPT.archs.tm2t_evaluator MovementConvEncoder
mGPT.archs.tm2t_evaluator MotionEncoderBiGRUCo
mGPT.archs.mgpt_vq VQVae
num_branches: 2
Applying rotation trick:  True
mGPT.archs.mgpt_lm MLM
Loading HumanML3D train â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 GB 0:00:00



Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00
Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00
Pointer Pointing at 0
Loading HumanML3D test â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.2/619.2 MB 0:00:00

LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name    â”ƒ Type        â”ƒ Params â”ƒ Mode  â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ metrics â”‚ BaseMetrics â”‚ 65.1 M â”‚ train â”‚
â”‚ 1 â”‚ vae     â”‚ VQVae       â”‚ 42.2 M â”‚ train â”‚
â”‚ 2 â”‚ lm      â”‚ MLM         â”‚  248 M â”‚ train â”‚
â”‚ 3 â”‚ _losses â”‚ ModuleDict  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 290 M                                                         
Non-trainable params: 65.1 M                                                    
Total params: 355 M                                                             
Total estimated model params size (MB): 1.4 K                                   
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
Pointer Pointing at 0
torch.optim AdamW
torch.optim.lr_scheduler CosineAnnealingLR
2025-02-21 16:50:36,465 Sanity checking ok.
/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
2025-02-21 16:50:39,175 Training started
2025-02-21 16:50:44,952 Epoch 0: 
2025-02-21 16:50:54,481 Epoch 1: loss_total 1.004e+00
2025-02-21 16:51:04,004 Epoch 2: loss_total 9.863e-01
2025-02-21 16:51:13,477 Epoch 3: loss_total 9.484e-01
2025-02-21 16:51:23,356 Epoch 4: loss_total 8.689e-01
2025-02-21 16:51:32,812 Epoch 5: loss_total 8.194e-01
2025-02-21 16:51:42,260 Epoch 6: loss_total 7.969e-01
2025-02-21 16:51:51,602 Epoch 7: loss_total 7.878e-01
2025-02-21 16:52:01,536 Epoch 8: loss_total 7.806e-01
2025-02-21 16:52:52,599 Epoch 9: loss_total 7.441e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:53:13,569 Epoch 10: loss_total 7.441e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:53:23,501 Epoch 11: loss_total 7.401e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:53:33,311 Epoch 12: loss_total 7.239e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:53:42,891 Epoch 13: loss_total 7.284e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:53:52,488 Epoch 14: loss_total 7.238e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:54:02,073 Epoch 15: loss_total 7.014e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:54:11,670 Epoch 16: loss_total 6.847e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:54:21,330 Epoch 17: loss_total 6.799e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:54:30,937 Epoch 18: loss_total 6.767e-01   FID 3.515e+01   Diversity 5.865e+00
2025-02-21 16:55:20,722 Epoch 19: loss_total 6.664e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:55:42,317 Epoch 20: loss_total 6.664e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:55:51,810 Epoch 21: loss_total 6.468e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:56:01,394 Epoch 22: loss_total 6.563e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:56:11,749 Epoch 23: loss_total 6.519e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:56:21,371 Epoch 24: loss_total 6.555e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:56:30,970 Epoch 25: loss_total 6.501e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:56:40,493 Epoch 26: loss_total 6.491e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:56:50,098 Epoch 27: loss_total 6.333e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:56:59,729 Epoch 28: loss_total 6.354e-01   FID 2.781e+01   Diversity 6.957e+00
2025-02-21 16:57:50,740 Epoch 29: loss_total 6.223e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 16:58:11,985 Epoch 30: loss_total 6.223e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 16:58:21,595 Epoch 31: loss_total 6.300e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 16:58:31,483 Epoch 32: loss_total 6.328e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 16:58:41,078 Epoch 33: loss_total 6.192e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 16:58:50,713 Epoch 34: loss_total 6.146e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 16:59:00,365 Epoch 35: loss_total 6.177e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 16:59:09,996 Epoch 36: loss_total 6.042e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 16:59:19,612 Epoch 37: loss_total 6.094e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 16:59:29,225 Epoch 38: loss_total 5.931e-01   FID 1.834e+01   Diversity 7.200e+00
2025-02-21 17:00:20,222 Epoch 39: loss_total 5.934e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:00:41,850 Epoch 40: loss_total 5.934e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:00:51,691 Epoch 41: loss_total 5.853e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:01:01,293 Epoch 42: loss_total 5.886e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:01:10,881 Epoch 43: loss_total 5.762e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:01:20,826 Epoch 44: loss_total 5.768e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:01:30,338 Epoch 45: loss_total 5.701e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:01:40,328 Epoch 46: loss_total 5.596e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:01:50,130 Epoch 47: loss_total 5.687e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:01:59,813 Epoch 48: loss_total 5.643e-01   FID 1.645e+01   Diversity 7.560e+00
2025-02-21 17:02:50,315 Epoch 49: loss_total 5.533e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:03:11,425 Epoch 50: loss_total 5.533e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:03:20,939 Epoch 51: loss_total 5.412e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:03:30,641 Epoch 52: loss_total 5.571e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:03:40,615 Epoch 53: loss_total 5.423e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:03:50,277 Epoch 54: loss_total 5.377e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:04:00,225 Epoch 55: loss_total 5.381e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:04:10,223 Epoch 56: loss_total 5.334e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:04:19,817 Epoch 57: loss_total 5.322e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:04:29,549 Epoch 58: loss_total 5.227e-01   FID 7.497e+00   Diversity 7.820e+00
2025-02-21 17:05:23,026 Epoch 59: loss_total 5.234e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:05:44,952 Epoch 60: loss_total 5.234e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:05:54,574 Epoch 61: loss_total 5.070e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:06:04,193 Epoch 62: loss_total 5.126e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:06:13,875 Epoch 63: loss_total 5.037e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:06:23,544 Epoch 64: loss_total 5.009e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:06:33,173 Epoch 65: loss_total 4.973e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:06:42,840 Epoch 66: loss_total 4.933e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:06:52,454 Epoch 67: loss_total 4.906e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:07:02,501 Epoch 68: loss_total 4.851e-01   FID 5.895e+00   Diversity 8.742e+00
2025-02-21 17:07:54,906 Epoch 69: loss_total 4.745e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:08:15,960 Epoch 70: loss_total 4.745e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:08:26,040 Epoch 71: loss_total 4.712e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:08:35,635 Epoch 72: loss_total 4.772e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:08:45,178 Epoch 73: loss_total 4.587e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:08:54,828 Epoch 74: loss_total 4.639e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:09:04,553 Epoch 75: loss_total 4.610e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:09:14,204 Epoch 76: loss_total 4.518e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:09:24,065 Epoch 77: loss_total 4.517e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:09:33,747 Epoch 78: loss_total 4.517e-01   FID 5.460e+00   Diversity 8.580e+00
2025-02-21 17:10:25,443 Epoch 79: loss_total 4.435e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:10:48,126 Epoch 80: loss_total 4.435e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:10:57,809 Epoch 81: loss_total 4.389e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:11:07,391 Epoch 82: loss_total 4.332e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:11:17,034 Epoch 83: loss_total 4.292e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:11:26,661 Epoch 84: loss_total 4.349e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:11:36,722 Epoch 85: loss_total 4.288e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:11:46,348 Epoch 86: loss_total 4.279e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:11:56,016 Epoch 87: loss_total 4.193e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:12:05,854 Epoch 88: loss_total 4.306e-01   FID 4.169e+00   Diversity 8.790e+00
2025-02-21 17:13:00,328 Epoch 89: loss_total 4.227e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:13:22,450 Epoch 90: loss_total 4.227e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:13:32,301 Epoch 91: loss_total 4.262e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:13:41,936 Epoch 92: loss_total 4.244e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:13:51,625 Epoch 93: loss_total 4.234e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:14:01,228 Epoch 94: loss_total 4.159e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:14:10,911 Epoch 95: loss_total 4.133e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:14:20,573 Epoch 96: loss_total 4.051e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:14:30,190 Epoch 97: loss_total 4.117e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:14:40,049 Epoch 98: loss_total 4.076e-01   FID 3.998e+00   Diversity 8.748e+00
2025-02-21 17:15:32,565 Epoch 99: loss_total 4.013e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:16:03,875 Epoch 100: loss_total 4.013e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:16:13,642 Epoch 101: loss_total 4.083e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:16:23,338 Epoch 102: loss_total 4.057e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:16:33,056 Epoch 103: loss_total 3.932e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:16:42,874 Epoch 104: loss_total 4.019e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:16:52,709 Epoch 105: loss_total 3.976e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:17:02,682 Epoch 106: loss_total 3.950e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:17:12,743 Epoch 107: loss_total 3.935e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:17:22,357 Epoch 108: loss_total 3.931e-01   FID 3.262e+00   Diversity 9.251e+00
2025-02-21 17:18:15,718 Epoch 109: loss_total 3.891e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:18:37,706 Epoch 110: loss_total 3.891e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:18:47,610 Epoch 111: loss_total 3.872e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:18:57,487 Epoch 112: loss_total 3.876e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:19:07,241 Epoch 113: loss_total 3.895e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:19:17,123 Epoch 114: loss_total 3.865e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:19:26,972 Epoch 115: loss_total 3.796e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:19:36,641 Epoch 116: loss_total 3.833e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:19:46,580 Epoch 117: loss_total 3.841e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:19:56,341 Epoch 118: loss_total 3.809e-01   FID 2.929e+00   Diversity 9.146e+00
2025-02-21 17:20:49,517 Epoch 119: loss_total 3.778e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:21:07,417 Epoch 120: loss_total 3.778e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:21:17,095 Epoch 121: loss_total 3.769e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:21:26,652 Epoch 122: loss_total 3.706e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:21:36,711 Epoch 123: loss_total 3.699e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:21:46,392 Epoch 124: loss_total 3.672e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:21:56,064 Epoch 125: loss_total 3.674e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:22:05,778 Epoch 126: loss_total 3.646e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:22:15,562 Epoch 127: loss_total 3.667e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:22:25,257 Epoch 128: loss_total 3.678e-01   FID 2.266e+00   Diversity 9.125e+00
2025-02-21 17:23:18,441 Epoch 129: loss_total 3.614e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:23:39,887 Epoch 130: loss_total 3.614e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:23:49,569 Epoch 131: loss_total 3.557e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:23:59,299 Epoch 132: loss_total 3.583e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:24:09,407 Epoch 133: loss_total 3.631e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:24:19,180 Epoch 134: loss_total 3.537e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:24:28,871 Epoch 135: loss_total 3.589e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:24:38,971 Epoch 136: loss_total 3.559e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:24:48,989 Epoch 137: loss_total 3.497e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:24:58,684 Epoch 138: loss_total 3.536e-01   FID 2.257e+00   Diversity 9.509e+00
2025-02-21 17:25:47,230 Epoch 139: loss_total 3.568e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:26:08,989 Epoch 140: loss_total 3.568e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:26:18,679 Epoch 141: loss_total 3.582e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:26:28,419 Epoch 142: loss_total 3.471e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:26:38,230 Epoch 143: loss_total 3.470e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:26:48,203 Epoch 144: loss_total 3.452e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:26:58,050 Epoch 145: loss_total 3.500e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:27:07,851 Epoch 146: loss_total 3.447e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:27:17,923 Epoch 147: loss_total 3.464e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:27:27,634 Epoch 148: loss_total 3.483e-01   FID 2.037e+00   Diversity 9.128e+00
2025-02-21 17:28:18,389 Epoch 149: loss_total 3.433e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:28:40,886 Epoch 150: loss_total 3.433e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:28:50,553 Epoch 151: loss_total 3.435e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:29:00,260 Epoch 152: loss_total 3.482e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:29:10,331 Epoch 153: loss_total 3.431e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:29:20,067 Epoch 154: loss_total 3.470e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:29:30,166 Epoch 155: loss_total 3.356e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:29:40,044 Epoch 156: loss_total 3.354e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:29:49,981 Epoch 157: loss_total 3.391e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:30:00,274 Epoch 158: loss_total 3.409e-01   FID 1.625e+00   Diversity 9.320e+00
2025-02-21 17:30:49,874 Epoch 159: loss_total 3.326e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:31:11,406 Epoch 160: loss_total 3.326e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:31:21,131 Epoch 161: loss_total 3.450e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:31:31,262 Epoch 162: loss_total 3.342e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:31:41,024 Epoch 163: loss_total 3.381e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:31:50,781 Epoch 164: loss_total 3.292e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:32:00,593 Epoch 165: loss_total 3.338e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:32:10,340 Epoch 166: loss_total 3.334e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:32:20,350 Epoch 167: loss_total 3.323e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:32:30,082 Epoch 168: loss_total 3.294e-01   FID 1.618e+00   Diversity 9.307e+00
2025-02-21 17:33:20,027 Epoch 169: loss_total 3.322e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:33:38,003 Epoch 170: loss_total 3.322e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:33:48,143 Epoch 171: loss_total 3.338e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:33:58,099 Epoch 172: loss_total 3.277e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:34:08,136 Epoch 173: loss_total 3.315e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:34:17,837 Epoch 174: loss_total 3.262e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:34:27,848 Epoch 175: loss_total 3.235e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:34:37,629 Epoch 176: loss_total 3.270e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:34:47,691 Epoch 177: loss_total 3.205e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:34:57,405 Epoch 178: loss_total 3.240e-01   FID 1.759e+00   Diversity 9.081e+00
2025-02-21 17:35:47,277 Epoch 179: loss_total 3.234e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:36:05,193 Epoch 180: loss_total 3.234e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:36:14,912 Epoch 181: loss_total 3.243e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:36:24,649 Epoch 182: loss_total 3.227e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:36:34,669 Epoch 183: loss_total 3.205e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:36:44,483 Epoch 184: loss_total 3.244e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:36:54,171 Epoch 185: loss_total 3.261e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:37:03,950 Epoch 186: loss_total 3.248e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:37:13,796 Epoch 187: loss_total 3.215e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:37:23,621 Epoch 188: loss_total 3.191e-01   FID 1.748e+00   Diversity 9.270e+00
2025-02-21 17:38:14,211 Epoch 189: loss_total 3.208e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:38:36,958 Epoch 190: loss_total 3.208e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:38:46,726 Epoch 191: loss_total 3.216e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:38:56,692 Epoch 192: loss_total 3.243e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:39:06,488 Epoch 193: loss_total 3.179e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:39:16,630 Epoch 194: loss_total 3.190e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:39:26,449 Epoch 195: loss_total 3.212e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:39:36,208 Epoch 196: loss_total 3.162e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:39:46,500 Epoch 197: loss_total 3.143e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:39:56,267 Epoch 198: loss_total 3.177e-01   FID 1.526e+00   Diversity 9.303e+00
2025-02-21 17:40:46,730 Epoch 199: loss_total 3.209e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:41:17,924 Epoch 200: loss_total 3.209e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:41:27,843 Epoch 201: loss_total 3.255e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:41:37,876 Epoch 202: loss_total 3.225e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:41:48,092 Epoch 203: loss_total 3.183e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:41:58,064 Epoch 204: loss_total 3.214e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:42:07,808 Epoch 205: loss_total 3.198e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:42:17,635 Epoch 206: loss_total 3.136e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:42:27,474 Epoch 207: loss_total 3.231e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:42:37,300 Epoch 208: loss_total 3.222e-01   FID 1.413e+00   Diversity 9.087e+00
2025-02-21 17:43:27,114 Epoch 209: loss_total 3.239e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:43:44,958 Epoch 210: loss_total 3.239e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:43:55,015 Epoch 211: loss_total 3.170e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:44:04,838 Epoch 212: loss_total 3.165e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:44:14,623 Epoch 213: loss_total 3.152e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:44:24,510 Epoch 214: loss_total 3.132e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:44:34,259 Epoch 215: loss_total 3.152e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:44:44,038 Epoch 216: loss_total 3.157e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:44:54,302 Epoch 217: loss_total 3.107e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:45:04,138 Epoch 218: loss_total 3.156e-01   FID 1.421e+00   Diversity 9.455e+00
2025-02-21 17:45:54,410 Epoch 219: loss_total 3.154e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:46:16,575 Epoch 220: loss_total 3.154e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:46:26,404 Epoch 221: loss_total 3.131e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:46:36,497 Epoch 222: loss_total 3.114e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:46:46,304 Epoch 223: loss_total 3.179e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:46:56,098 Epoch 224: loss_total 3.072e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:47:06,214 Epoch 225: loss_total 3.132e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:47:16,246 Epoch 226: loss_total 3.151e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:47:25,987 Epoch 227: loss_total 3.159e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:47:35,762 Epoch 228: loss_total 3.132e-01   FID 1.396e+00   Diversity 9.509e+00
2025-02-21 17:48:25,645 Epoch 229: loss_total 3.141e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:48:43,697 Epoch 230: loss_total 3.141e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:48:53,506 Epoch 231: loss_total 3.126e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:49:03,332 Epoch 232: loss_total 3.146e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:49:13,092 Epoch 233: loss_total 3.120e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:49:22,944 Epoch 234: loss_total 3.076e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:49:32,728 Epoch 235: loss_total 3.136e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:49:42,577 Epoch 236: loss_total 3.171e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:49:52,317 Epoch 237: loss_total 3.138e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:50:02,119 Epoch 238: loss_total 3.142e-01   FID 1.462e+00   Diversity 9.326e+00
2025-02-21 17:50:52,863 Epoch 239: loss_total 3.102e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:51:10,469 Epoch 240: loss_total 3.102e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:51:20,206 Epoch 241: loss_total 3.131e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:51:29,983 Epoch 242: loss_total 3.077e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:51:40,009 Epoch 243: loss_total 3.118e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:51:49,790 Epoch 244: loss_total 3.103e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:51:59,517 Epoch 245: loss_total 3.093e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:52:09,595 Epoch 246: loss_total 3.073e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:52:19,418 Epoch 247: loss_total 3.064e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:52:29,427 Epoch 248: loss_total 3.066e-01   FID 1.161e+00   Diversity 9.177e+00
2025-02-21 17:53:19,400 Epoch 249: loss_total 3.133e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:53:33,078 Epoch 250: loss_total 3.133e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:53:43,299 Epoch 251: loss_total 3.075e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:53:53,104 Epoch 252: loss_total 3.067e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:54:03,006 Epoch 253: loss_total 3.135e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:54:12,818 Epoch 254: loss_total 3.044e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:54:22,672 Epoch 255: loss_total 3.038e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:54:32,541 Epoch 256: loss_total 3.041e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:54:42,758 Epoch 257: loss_total 3.077e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:54:52,603 Epoch 258: loss_total 3.072e-01   FID 1.357e+00   Diversity 9.524e+00
2025-02-21 17:55:42,597 Epoch 259: loss_total 3.038e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:56:01,181 Epoch 260: loss_total 3.038e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:56:10,962 Epoch 261: loss_total 3.073e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:56:20,814 Epoch 262: loss_total 3.032e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:56:30,810 Epoch 263: loss_total 3.047e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:56:40,649 Epoch 264: loss_total 3.048e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:56:50,426 Epoch 265: loss_total 3.074e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:57:00,159 Epoch 266: loss_total 3.086e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:57:10,114 Epoch 267: loss_total 3.075e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:57:20,697 Epoch 268: loss_total 3.021e-01   FID 1.200e+00   Diversity 9.627e+00
2025-02-21 17:58:10,527 Epoch 269: loss_total 3.035e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 17:58:24,241 Epoch 270: loss_total 3.035e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 17:58:34,074 Epoch 271: loss_total 3.039e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 17:58:44,351 Epoch 272: loss_total 3.024e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 17:58:54,526 Epoch 273: loss_total 3.026e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 17:59:04,367 Epoch 274: loss_total 3.024e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 17:59:14,203 Epoch 275: loss_total 3.082e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 17:59:24,009 Epoch 276: loss_total 3.077e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 17:59:33,806 Epoch 277: loss_total 3.056e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 17:59:43,626 Epoch 278: loss_total 3.045e-01   FID 1.257e+00   Diversity 9.144e+00
2025-02-21 18:00:34,266 Epoch 279: loss_total 3.046e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:00:47,909 Epoch 280: loss_total 3.046e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:00:57,724 Epoch 281: loss_total 3.097e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:01:07,513 Epoch 282: loss_total 3.033e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:01:17,480 Epoch 283: loss_total 3.106e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:01:27,595 Epoch 284: loss_total 3.072e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:01:37,397 Epoch 285: loss_total 2.984e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:01:47,216 Epoch 286: loss_total 3.042e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:01:57,073 Epoch 287: loss_total 3.054e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:02:07,310 Epoch 288: loss_total 3.033e-01   FID 1.420e+00   Diversity 9.258e+00
2025-02-21 18:02:58,583 Epoch 289: loss_total 3.000e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:03:16,349 Epoch 290: loss_total 3.000e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:03:26,201 Epoch 291: loss_total 3.037e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:03:36,218 Epoch 292: loss_total 3.026e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:03:45,958 Epoch 293: loss_total 3.008e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:03:55,721 Epoch 294: loss_total 2.985e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:04:05,579 Epoch 295: loss_total 3.013e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:04:15,526 Epoch 296: loss_total 3.049e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:04:25,613 Epoch 297: loss_total 3.034e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:04:35,464 Epoch 298: loss_total 3.064e-01   FID 1.085e+00   Diversity 9.209e+00
2025-02-21 18:05:26,259 Epoch 299: loss_total 2.947e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:05:57,700 Epoch 300: loss_total 2.947e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:06:07,536 Epoch 301: loss_total 3.031e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:06:17,231 Epoch 302: loss_total 3.055e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:06:26,987 Epoch 303: loss_total 3.038e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:06:36,836 Epoch 304: loss_total 2.989e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:06:46,639 Epoch 305: loss_total 3.013e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:06:56,501 Epoch 306: loss_total 2.998e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:07:06,377 Epoch 307: loss_total 2.992e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:07:16,148 Epoch 308: loss_total 3.024e-01   FID 1.021e+00   Diversity 9.477e+00
2025-02-21 18:08:07,014 Epoch 309: loss_total 3.063e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:08:20,592 Epoch 310: loss_total 3.063e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:08:30,343 Epoch 311: loss_total 3.037e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:08:40,315 Epoch 312: loss_total 3.017e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:08:50,593 Epoch 313: loss_total 2.965e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:09:00,999 Epoch 314: loss_total 2.983e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:09:11,019 Epoch 315: loss_total 2.934e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:09:20,820 Epoch 316: loss_total 2.983e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:09:30,557 Epoch 317: loss_total 3.009e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:09:40,376 Epoch 318: loss_total 3.007e-01   FID 1.175e+00   Diversity 9.214e+00
2025-02-21 18:10:31,245 Epoch 319: loss_total 3.015e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:10:45,103 Epoch 320: loss_total 3.015e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:10:54,849 Epoch 321: loss_total 3.037e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:11:04,956 Epoch 322: loss_total 3.014e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:11:14,809 Epoch 323: loss_total 2.991e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:11:24,600 Epoch 324: loss_total 3.007e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:11:34,445 Epoch 325: loss_total 3.003e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:11:44,307 Epoch 326: loss_total 3.026e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:11:54,092 Epoch 327: loss_total 3.047e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:12:04,157 Epoch 328: loss_total 2.967e-01   FID 1.176e+00   Diversity 9.411e+00
2025-02-21 18:12:54,985 Epoch 329: loss_total 2.990e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:13:08,649 Epoch 330: loss_total 2.990e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:13:18,339 Epoch 331: loss_total 2.998e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:13:28,054 Epoch 332: loss_total 2.969e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:13:38,031 Epoch 333: loss_total 2.970e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:13:47,844 Epoch 334: loss_total 3.014e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:13:58,001 Epoch 335: loss_total 2.963e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:14:07,819 Epoch 336: loss_total 2.954e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:14:17,599 Epoch 337: loss_total 2.971e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:14:27,376 Epoch 338: loss_total 2.953e-01   FID 1.111e+00   Diversity 9.684e+00
2025-02-21 18:15:17,451 Epoch 339: loss_total 3.011e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:15:35,431 Epoch 340: loss_total 3.011e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:15:45,185 Epoch 341: loss_total 2.920e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:15:54,959 Epoch 342: loss_total 2.956e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:16:04,999 Epoch 343: loss_total 2.959e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:16:14,787 Epoch 344: loss_total 2.966e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:16:24,566 Epoch 345: loss_total 2.980e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:16:34,296 Epoch 346: loss_total 2.961e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:16:44,153 Epoch 347: loss_total 2.958e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:16:53,972 Epoch 348: loss_total 2.958e-01   FID 1.031e+00   Diversity 9.089e+00
2025-02-21 18:17:43,341 Epoch 349: loss_total 2.954e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:18:05,176 Epoch 350: loss_total 2.954e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:18:14,931 Epoch 351: loss_total 2.986e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:18:24,715 Epoch 352: loss_total 2.939e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:18:34,503 Epoch 353: loss_total 2.932e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:18:44,212 Epoch 354: loss_total 2.951e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:18:54,077 Epoch 355: loss_total 2.941e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:19:03,946 Epoch 356: loss_total 2.977e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:19:14,069 Epoch 357: loss_total 2.896e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:19:23,999 Epoch 358: loss_total 2.933e-01   FID 8.301e-01   Diversity 9.185e+00
2025-02-21 18:20:14,017 Epoch 359: loss_total 2.897e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:20:27,802 Epoch 360: loss_total 2.897e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:20:37,918 Epoch 361: loss_total 2.966e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:20:48,009 Epoch 362: loss_total 2.964e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:20:57,998 Epoch 363: loss_total 2.963e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:21:07,908 Epoch 364: loss_total 2.934e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:21:17,702 Epoch 365: loss_total 2.941e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:21:27,524 Epoch 366: loss_total 2.961e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:21:37,316 Epoch 367: loss_total 2.946e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:21:47,097 Epoch 368: loss_total 2.987e-01   FID 1.118e+00   Diversity 9.271e+00
2025-02-21 18:22:36,379 Epoch 369: loss_total 2.947e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:22:50,079 Epoch 370: loss_total 2.947e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:22:59,934 Epoch 371: loss_total 2.968e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:23:10,152 Epoch 372: loss_total 2.946e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:23:20,015 Epoch 373: loss_total 2.942e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:23:29,753 Epoch 374: loss_total 2.898e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:23:39,613 Epoch 375: loss_total 2.928e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:23:49,505 Epoch 376: loss_total 2.932e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:23:59,205 Epoch 377: loss_total 2.917e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:24:08,909 Epoch 378: loss_total 2.945e-01   FID 1.135e+00   Diversity 9.646e+00
2025-02-21 18:25:00,156 Epoch 379: loss_total 2.942e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:25:13,903 Epoch 380: loss_total 2.942e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:25:23,661 Epoch 381: loss_total 2.988e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:25:33,420 Epoch 382: loss_total 2.927e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:25:43,253 Epoch 383: loss_total 2.935e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:25:53,186 Epoch 384: loss_total 2.993e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:26:02,871 Epoch 385: loss_total 2.934e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:26:12,936 Epoch 386: loss_total 2.929e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:26:22,734 Epoch 387: loss_total 2.933e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:26:32,911 Epoch 388: loss_total 2.958e-01   FID 9.311e-01   Diversity 9.492e+00
2025-02-21 18:27:22,768 Epoch 389: loss_total 2.929e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:27:36,811 Epoch 390: loss_total 2.929e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:27:46,616 Epoch 391: loss_total 2.927e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:27:56,383 Epoch 392: loss_total 2.974e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:28:06,432 Epoch 393: loss_total 2.933e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:28:16,211 Epoch 394: loss_total 2.956e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:28:26,080 Epoch 395: loss_total 2.916e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:28:35,862 Epoch 396: loss_total 2.922e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:28:45,723 Epoch 397: loss_total 2.935e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:28:55,579 Epoch 398: loss_total 2.958e-01   FID 1.023e+00   Diversity 9.577e+00
2025-02-21 18:29:45,570 Epoch 399: loss_total 2.977e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:30:08,633 Epoch 400: loss_total 2.977e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:30:19,859 Epoch 401: loss_total 2.909e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:30:29,695 Epoch 402: loss_total 2.945e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:30:39,388 Epoch 403: loss_total 2.920e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:30:49,114 Epoch 404: loss_total 2.903e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:30:58,998 Epoch 405: loss_total 2.922e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:31:08,845 Epoch 406: loss_total 2.894e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:31:18,646 Epoch 407: loss_total 2.961e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:31:28,483 Epoch 408: loss_total 2.966e-01   FID 1.023e+00   Diversity 9.154e+00
2025-02-21 18:32:18,534 Epoch 409: loss_total 2.937e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:32:36,482 Epoch 410: loss_total 2.937e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:32:46,272 Epoch 411: loss_total 2.931e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:32:56,052 Epoch 412: loss_total 2.903e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:33:05,884 Epoch 413: loss_total 2.917e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:33:15,724 Epoch 414: loss_total 2.903e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:33:25,563 Epoch 415: loss_total 2.914e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:33:35,692 Epoch 416: loss_total 2.908e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:33:46,158 Epoch 417: loss_total 2.885e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:33:55,949 Epoch 418: loss_total 2.851e-01   FID 7.587e-01   Diversity 9.042e+00
2025-02-21 18:34:46,520 Epoch 419: loss_total 2.872e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:35:00,172 Epoch 420: loss_total 2.872e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:35:09,949 Epoch 421: loss_total 2.888e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:35:19,739 Epoch 422: loss_total 2.915e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:35:29,605 Epoch 423: loss_total 2.896e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:35:39,372 Epoch 424: loss_total 2.885e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:35:49,200 Epoch 425: loss_total 2.934e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:35:59,707 Epoch 426: loss_total 2.884e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:36:09,438 Epoch 427: loss_total 2.861e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:36:19,303 Epoch 428: loss_total 2.854e-01   FID 9.714e-01   Diversity 9.592e+00
2025-02-21 18:37:08,274 Epoch 429: loss_total 2.874e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:37:22,493 Epoch 430: loss_total 2.874e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:37:32,579 Epoch 431: loss_total 2.916e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:37:42,312 Epoch 432: loss_total 2.906e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:37:52,214 Epoch 433: loss_total 2.940e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:38:02,116 Epoch 434: loss_total 2.881e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:38:12,087 Epoch 435: loss_total 2.865e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:38:21,977 Epoch 436: loss_total 2.907e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:38:31,872 Epoch 437: loss_total 2.895e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:38:41,763 Epoch 438: loss_total 2.922e-01   FID 8.950e-01   Diversity 9.395e+00
2025-02-21 18:39:31,706 Epoch 439: loss_total 2.909e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:39:45,384 Epoch 440: loss_total 2.909e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:39:55,292 Epoch 441: loss_total 2.904e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:40:05,151 Epoch 442: loss_total 2.877e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:40:15,005 Epoch 443: loss_total 2.886e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:40:25,067 Epoch 444: loss_total 2.892e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:40:34,930 Epoch 445: loss_total 2.915e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:40:45,136 Epoch 446: loss_total 2.904e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:40:54,964 Epoch 447: loss_total 2.878e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:41:05,364 Epoch 448: loss_total 2.857e-01   FID 9.487e-01   Diversity 9.414e+00
2025-02-21 18:41:55,284 Epoch 449: loss_total 2.901e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:42:13,250 Epoch 450: loss_total 2.901e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:42:23,078 Epoch 451: loss_total 2.867e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:42:33,033 Epoch 452: loss_total 2.858e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:42:42,758 Epoch 453: loss_total 2.930e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:42:52,487 Epoch 454: loss_total 2.877e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:43:02,251 Epoch 455: loss_total 2.862e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:43:12,063 Epoch 456: loss_total 2.855e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:43:21,879 Epoch 457: loss_total 2.884e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:43:31,701 Epoch 458: loss_total 2.910e-01   FID 7.153e-01   Diversity 9.612e+00
2025-02-21 18:44:22,659 Epoch 459: loss_total 2.910e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:44:36,754 Epoch 460: loss_total 2.910e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:44:46,577 Epoch 461: loss_total 2.920e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:44:56,454 Epoch 462: loss_total 2.886e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:45:06,352 Epoch 463: loss_total 2.901e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:45:16,235 Epoch 464: loss_total 2.879e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:45:26,089 Epoch 465: loss_total 2.851e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:45:36,100 Epoch 466: loss_total 2.908e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:45:45,941 Epoch 467: loss_total 2.852e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:45:55,749 Epoch 468: loss_total 2.889e-01   FID 9.111e-01   Diversity 9.645e+00
2025-02-21 18:46:45,409 Epoch 469: loss_total 2.860e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:46:59,181 Epoch 470: loss_total 2.860e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:47:09,259 Epoch 471: loss_total 2.852e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:47:19,238 Epoch 472: loss_total 2.798e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:47:29,073 Epoch 473: loss_total 2.854e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:47:39,407 Epoch 474: loss_total 2.879e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:47:49,507 Epoch 475: loss_total 2.881e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:47:59,345 Epoch 476: loss_total 2.922e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:48:09,340 Epoch 477: loss_total 2.872e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:48:19,193 Epoch 478: loss_total 2.880e-01   FID 9.187e-01   Diversity 9.388e+00
2025-02-21 18:49:09,714 Epoch 479: loss_total 2.862e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:49:23,544 Epoch 480: loss_total 2.862e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:49:33,650 Epoch 481: loss_total 2.862e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:49:43,909 Epoch 482: loss_total 2.835e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:49:53,702 Epoch 483: loss_total 2.848e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:50:03,928 Epoch 484: loss_total 2.917e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:50:14,135 Epoch 485: loss_total 2.846e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:50:24,016 Epoch 486: loss_total 2.837e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:50:33,867 Epoch 487: loss_total 2.817e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:50:43,781 Epoch 488: loss_total 2.839e-01   FID 8.005e-01   Diversity 9.162e+00
2025-02-21 18:51:33,908 Epoch 489: loss_total 2.873e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:51:47,914 Epoch 490: loss_total 2.873e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:51:57,671 Epoch 491: loss_total 2.873e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:52:07,593 Epoch 492: loss_total 2.813e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:52:17,376 Epoch 493: loss_total 2.893e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:52:27,253 Epoch 494: loss_total 2.854e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:52:37,495 Epoch 495: loss_total 2.869e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:52:47,493 Epoch 496: loss_total 2.860e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:52:57,387 Epoch 497: loss_total 2.897e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:53:07,574 Epoch 498: loss_total 2.856e-01   FID 7.730e-01   Diversity 9.164e+00
2025-02-21 18:53:57,460 Epoch 499: loss_total 2.858e-01   FID 8.259e-01   Diversity 9.402e+00
`Trainer.fit` stopped: `max_epochs=500` reached.
2025-02-21 18:54:15,229 Training done
Epoch 499/499 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0:00:05 â€¢ 0:00:00 0.93it/s 

Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/trial-week/MotionGPT/train.py", line 94, in <module>
    main()
  File "/root/trial-week/MotionGPT/train.py", line 85, in main
    trainer.fit(model, datamodule=datamodule)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 999, in _run
    call._call_teardown_hook(self)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 131, in _call_teardown_hook
    logger.finalize("success")
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py", line 644, in finalize
    self._scan_and_log_checkpoints(self._checkpoint_callback)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py", line 674, in _scan_and_log_checkpoints
    artifact.add_file(p, name="model.ckpt")
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/wandb/sdk/artifacts/_validators.py", line 115, in wrapper
    return method(self, *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/wandb/sdk/artifacts/artifact.py", line 1312, in add_file
    return self._add_local_file(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/wandb/sdk/artifacts/artifact.py", line 1577, in _add_local_file
    shutil.copyfile(path, staging_path)
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/shutil.py", line 267, in copyfile
    _fastcopy_sendfile(fsrc, fdst)
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/shutil.py", line 156, in _fastcopy_sendfile
    raise err from None
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/shutil.py", line 142, in _fastcopy_sendfile
    sent = os.sendfile(outfd, infd, offset, blocksize)
OSError: [Errno 28] No space left on device: '/workspace/experiments/mgpt/Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs/checkpoints/epoch=499.ckpt' -> '/root/.local/share/wandb/artifacts/staging/tmplon46nih'
Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/trial-week/MotionGPT/train.py", line 94, in <module>
    main()
  File "/root/trial-week/MotionGPT/train.py", line 85, in main
    trainer.fit(model, datamodule=datamodule)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 999, in _run
    call._call_teardown_hook(self)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 131, in _call_teardown_hook
    logger.finalize("success")
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py", line 644, in finalize
    self._scan_and_log_checkpoints(self._checkpoint_callback)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py", line 674, in _scan_and_log_checkpoints
    artifact.add_file(p, name="model.ckpt")
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/wandb/sdk/artifacts/_validators.py", line 115, in wrapper
    return method(self, *args, **kwargs)
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/wandb/sdk/artifacts/artifact.py", line 1312, in add_file
    return self._add_local_file(
  File "/root/trial-week/MotionGPT/.venv/lib/python3.10/site-packages/wandb/sdk/artifacts/artifact.py", line 1577, in _add_local_file
    shutil.copyfile(path, staging_path)
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/shutil.py", line 267, in copyfile
    _fastcopy_sendfile(fsrc, fdst)
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/shutil.py", line 156, in _fastcopy_sendfile
    raise err from None
  File "/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/shutil.py", line 142, in _fastcopy_sendfile
    sent = os.sendfile(outfd, infd, offset, blocksize)
OSError: [Errno 28] No space left on device: '/workspace/experiments/mgpt/Parallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs/checkpoints/epoch=499.ckpt' -> '/root/.local/share/wandb/artifacts/staging/tmplon46nih'
[1;34mwandb[0m: 
[1;34mwandb[0m: ğŸš€ View run [33mParallel_VQVAE_Feature_Extraction_RotationTrick_300_epochs[0m at: [34mhttps://wandb.ai/irizar-xabier-finscout/motiongpt/runs/7yhuq4v6[0m
